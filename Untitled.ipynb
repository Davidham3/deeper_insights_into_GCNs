{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import gluon\n",
    "from mxnet import nd\n",
    "from mxnet import autograd\n",
    "from mxnet.gluon import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse.linalg.eigen.arpack import eigsh\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = mx.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class gcn_layer(nn.HybridBlock):\n",
    "    def __init__(self, num_of_filters, **kwargs):\n",
    "        super(gcn_layer, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.fc = nn.Dense(num_of_filters)\n",
    "        \n",
    "    def hybrid_forward(self, F, x, A_):\n",
    "        '''\n",
    "        Parameters\n",
    "        ----------\n",
    "        A_, D^{-1/2} A D^{-1/2}\n",
    "        '''\n",
    "        return self.fc(F.dot(A_, x))\n",
    "        \n",
    "        \n",
    "class GCN(nn.HybridBlock):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(GCN, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.gcn1 = gcn_layer(256)\n",
    "            self.gcn2 = gcn_layer(7)\n",
    "        \n",
    "    def hybrid_forward(self, F, x, A_):\n",
    "        return self.gcn2(F.relu(self.gcn1(x, A_)), A_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_index_file(filename):\n",
    "    \"\"\"Parse index file.\"\"\"\n",
    "    index = []\n",
    "    for line in open(filename):\n",
    "        index.append(int(line.strip()))\n",
    "    return index\n",
    "\n",
    "\n",
    "def sample_mask(idx, l):\n",
    "    \"\"\"Create mask.\"\"\"\n",
    "    mask = np.zeros(l)\n",
    "    mask[idx] = 1\n",
    "    return np.array(mask, dtype=np.bool)\n",
    "\n",
    "def load_data(dataset_str):\n",
    "    \"\"\"\n",
    "    Loads input data from gcn/data directory\n",
    "    ind.dataset_str.x => the feature vectors of the training instances as scipy.sparse.csr.csr_matrix object;\n",
    "    ind.dataset_str.tx => the feature vectors of the test instances as scipy.sparse.csr.csr_matrix object;\n",
    "    ind.dataset_str.allx => the feature vectors of both labeled and unlabeled training instances\n",
    "        (a superset of ind.dataset_str.x) as scipy.sparse.csr.csr_matrix object;\n",
    "    ind.dataset_str.y => the one-hot labels of the labeled training instances as numpy.ndarray object;\n",
    "    ind.dataset_str.ty => the one-hot labels of the test instances as numpy.ndarray object;\n",
    "    ind.dataset_str.ally => the labels for instances in ind.dataset_str.allx as numpy.ndarray object;\n",
    "    ind.dataset_str.graph => a dict in the format {index: [index_of_neighbor_nodes]} as collections.defaultdict\n",
    "        object;\n",
    "    ind.dataset_str.test.index => the indices of test instances in graph, for the inductive setting as list object.\n",
    "    All objects above must be saved using python pickle module.\n",
    "    :param dataset_str: Dataset name\n",
    "    :return: All data input files loaded (as well the training/test data).\n",
    "    \"\"\"\n",
    "    names = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'graph']\n",
    "    objects = []\n",
    "    for i in range(len(names)):\n",
    "        with open(\"data/ind.{}.{}\".format(dataset_str, names[i]), 'rb') as f:\n",
    "            if sys.version_info > (3, 0):\n",
    "                objects.append(pkl.load(f, encoding='latin1'))\n",
    "            else:\n",
    "                objects.append(pkl.load(f))\n",
    "\n",
    "    x, y, tx, ty, allx, ally, graph = tuple(objects)\n",
    "    test_idx_reorder = parse_index_file(\"data/ind.{}.test.index\".format(dataset_str))\n",
    "    test_idx_range = np.sort(test_idx_reorder)\n",
    "\n",
    "    if dataset_str == 'citeseer':\n",
    "        # Fix citeseer dataset (there are some isolated nodes in the graph)\n",
    "        # Find isolated nodes, add them as zero-vecs into the right position\n",
    "        test_idx_range_full = range(min(test_idx_reorder), max(test_idx_reorder)+1)\n",
    "        tx_extended = sp.lil_matrix((len(test_idx_range_full), x.shape[1]))\n",
    "        tx_extended[test_idx_range-min(test_idx_range), :] = tx\n",
    "        tx = tx_extended\n",
    "        ty_extended = np.zeros((len(test_idx_range_full), y.shape[1]))\n",
    "        ty_extended[test_idx_range-min(test_idx_range), :] = ty\n",
    "        ty = ty_extended\n",
    "\n",
    "    features = sp.vstack((allx, tx)).tolil()\n",
    "    features[test_idx_reorder, :] = features[test_idx_range, :]\n",
    "    adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))\n",
    "\n",
    "    labels = np.vstack((ally, ty))\n",
    "    labels[test_idx_reorder, :] = labels[test_idx_range, :]\n",
    "\n",
    "    idx_test = test_idx_range.tolist()\n",
    "    idx_train = range(len(y))\n",
    "    idx_val = range(len(y), len(y)+500)\n",
    "\n",
    "    train_mask = sample_mask(idx_train, labels.shape[0])\n",
    "    val_mask = sample_mask(idx_val, labels.shape[0])\n",
    "    test_mask = sample_mask(idx_test, labels.shape[0])\n",
    "\n",
    "    y_train = np.zeros(labels.shape)\n",
    "    y_val = np.zeros(labels.shape)\n",
    "    y_test = np.zeros(labels.shape)\n",
    "    y_train[train_mask, :] = labels[train_mask, :]\n",
    "    y_val[val_mask, :] = labels[val_mask, :]\n",
    "    y_test[test_mask, :] = labels[test_mask, :]\n",
    "\n",
    "    return adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask = load_data('cora')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = nd.array(features.toarray(), ctx = ctx)\n",
    "y_train = nd.array(y_train, ctx = ctx)\n",
    "y_val = nd.array(y_val, ctx = ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2708, 2708)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_tilde = adj.toarray() + np.identity(adj.shape[0])\n",
    "D = A_tilde.sum(axis = 1)\n",
    "A_ = nd.array(np.diag(D ** -0.5).dot(A_tilde).dot(np.diag(D ** -0.5)), ctx = ctx)\n",
    "A_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.arange(len(A_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = GCN()\n",
    "net.initialize(ctx = ctx)\n",
    "net.hybridize()\n",
    "output = net(features, A_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate': 1e-3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.95\n",
      "validation loss 1.93\n",
      "\n",
      "training loss: 1.91\n",
      "validation loss 1.91\n",
      "\n",
      "training loss: 1.88\n",
      "validation loss 1.90\n",
      "\n",
      "training loss: 1.85\n",
      "validation loss 1.88\n",
      "\n",
      "training loss: 1.81\n",
      "validation loss 1.86\n",
      "\n",
      "training loss: 1.77\n",
      "validation loss 1.84\n",
      "\n",
      "training loss: 1.74\n",
      "validation loss 1.82\n",
      "\n",
      "training loss: 1.70\n",
      "validation loss 1.80\n",
      "\n",
      "training loss: 1.66\n",
      "validation loss 1.78\n",
      "\n",
      "training loss: 1.61\n",
      "validation loss 1.75\n",
      "\n",
      "training loss: 1.57\n",
      "validation loss 1.73\n",
      "\n",
      "training loss: 1.52\n",
      "validation loss 1.70\n",
      "\n",
      "training loss: 1.47\n",
      "validation loss 1.67\n",
      "\n",
      "training loss: 1.42\n",
      "validation loss 1.64\n",
      "\n",
      "training loss: 1.37\n",
      "validation loss 1.61\n",
      "\n",
      "training loss: 1.31\n",
      "validation loss 1.57\n",
      "\n",
      "training loss: 1.26\n",
      "validation loss 1.54\n",
      "\n",
      "training loss: 1.20\n",
      "validation loss 1.51\n",
      "\n",
      "training loss: 1.15\n",
      "validation loss 1.47\n",
      "\n",
      "training loss: 1.09\n",
      "validation loss 1.44\n",
      "\n",
      "training loss: 1.03\n",
      "validation loss 1.40\n",
      "\n",
      "training loss: 0.98\n",
      "validation loss 1.37\n",
      "\n",
      "training loss: 0.92\n",
      "validation loss 1.33\n",
      "\n",
      "training loss: 0.87\n",
      "validation loss 1.30\n",
      "\n",
      "training loss: 0.82\n",
      "validation loss 1.26\n",
      "\n",
      "training loss: 0.77\n",
      "validation loss 1.23\n",
      "\n",
      "training loss: 0.72\n",
      "validation loss 1.20\n",
      "\n",
      "training loss: 0.67\n",
      "validation loss 1.16\n",
      "\n",
      "training loss: 0.63\n",
      "validation loss 1.13\n",
      "\n",
      "training loss: 0.59\n",
      "validation loss 1.10\n",
      "\n",
      "training loss: 0.54\n",
      "validation loss 1.07\n",
      "\n",
      "training loss: 0.51\n",
      "validation loss 1.05\n",
      "\n",
      "training loss: 0.47\n",
      "validation loss 1.02\n",
      "\n",
      "training loss: 0.44\n",
      "validation loss 0.99\n",
      "\n",
      "training loss: 0.40\n",
      "validation loss 0.97\n",
      "\n",
      "training loss: 0.37\n",
      "validation loss 0.95\n",
      "\n",
      "training loss: 0.35\n",
      "validation loss 0.93\n",
      "\n",
      "training loss: 0.32\n",
      "validation loss 0.91\n",
      "\n",
      "training loss: 0.30\n",
      "validation loss 0.89\n",
      "\n",
      "training loss: 0.28\n",
      "validation loss 0.87\n",
      "\n",
      "training loss: 0.25\n",
      "validation loss 0.86\n",
      "\n",
      "training loss: 0.24\n",
      "validation loss 0.84\n",
      "\n",
      "training loss: 0.22\n",
      "validation loss 0.83\n",
      "\n",
      "training loss: 0.20\n",
      "validation loss 0.82\n",
      "\n",
      "training loss: 0.19\n",
      "validation loss 0.81\n",
      "\n",
      "training loss: 0.17\n",
      "validation loss 0.80\n",
      "\n",
      "training loss: 0.16\n",
      "validation loss 0.79\n",
      "\n",
      "training loss: 0.15\n",
      "validation loss 0.78\n",
      "\n",
      "training loss: 0.14\n",
      "validation loss 0.77\n",
      "\n",
      "training loss: 0.13\n",
      "validation loss 0.77\n",
      "\n",
      "training loss: 0.12\n",
      "validation loss 0.76\n",
      "\n",
      "training loss: 0.11\n",
      "validation loss 0.76\n",
      "\n",
      "training loss: 0.11\n",
      "validation loss 0.75\n",
      "\n",
      "training loss: 0.10\n",
      "validation loss 0.75\n",
      "\n",
      "training loss: 0.09\n",
      "validation loss 0.74\n",
      "\n",
      "training loss: 0.09\n",
      "validation loss 0.74\n",
      "\n",
      "training loss: 0.08\n",
      "validation loss 0.74\n",
      "\n",
      "training loss: 0.08\n",
      "validation loss 0.73\n",
      "\n",
      "training loss: 0.07\n",
      "validation loss 0.73\n",
      "\n",
      "training loss: 0.07\n",
      "validation loss 0.73\n",
      "\n",
      "training loss: 0.07\n",
      "validation loss 0.73\n",
      "\n",
      "training loss: 0.06\n",
      "validation loss 0.73\n",
      "\n",
      "training loss: 0.06\n",
      "validation loss 0.73\n",
      "\n",
      "training loss: 0.06\n",
      "validation loss 0.72\n",
      "\n",
      "training loss: 0.05\n",
      "validation loss 0.72\n",
      "\n",
      "training loss: 0.05\n",
      "validation loss 0.72\n",
      "\n",
      "training loss: 0.05\n",
      "validation loss 0.72\n",
      "\n",
      "training loss: 0.05\n",
      "validation loss 0.72\n",
      "\n",
      "training loss: 0.04\n",
      "validation loss 0.72\n",
      "\n",
      "training loss: 0.04\n",
      "validation loss 0.72\n",
      "\n",
      "training loss: 0.04\n",
      "validation loss 0.72\n",
      "\n",
      "training loss: 0.04\n",
      "validation loss 0.72\n",
      "\n",
      "training loss: 0.04\n",
      "validation loss 0.72\n",
      "\n",
      "training loss: 0.04\n",
      "validation loss 0.72\n",
      "\n",
      "training loss: 0.03\n",
      "validation loss 0.72\n",
      "\n",
      "training loss: 0.03\n",
      "validation loss 0.72\n",
      "\n",
      "training loss: 0.03\n",
      "validation loss 0.72\n",
      "\n",
      "training loss: 0.03\n",
      "validation loss 0.72\n",
      "\n",
      "training loss: 0.03\n",
      "validation loss 0.72\n",
      "\n",
      "training loss: 0.03\n",
      "validation loss 0.72\n",
      "\n",
      "training loss: 0.03\n",
      "validation loss 0.72\n",
      "\n",
      "training loss: 0.03\n",
      "validation loss 0.72\n",
      "\n",
      "training loss: 0.03\n",
      "validation loss 0.72\n",
      "\n",
      "training loss: 0.03\n",
      "validation loss 0.72\n",
      "\n",
      "training loss: 0.02\n",
      "validation loss 0.72\n",
      "\n",
      "training loss: 0.02\n",
      "validation loss 0.72\n",
      "\n",
      "training loss: 0.02\n",
      "validation loss 0.72\n",
      "\n",
      "training loss: 0.02\n",
      "validation loss 0.72\n",
      "\n",
      "training loss: 0.02\n",
      "validation loss 0.72\n",
      "\n",
      "training loss: 0.02\n",
      "validation loss 0.72\n",
      "\n",
      "training loss: 0.02\n",
      "validation loss 0.72\n",
      "\n",
      "training loss: 0.02\n",
      "validation loss 0.72\n",
      "\n",
      "training loss: 0.02\n",
      "validation loss 0.72\n",
      "\n",
      "training loss: 0.02\n",
      "validation loss 0.72\n",
      "\n",
      "training loss: 0.02\n",
      "validation loss 0.72\n",
      "\n",
      "training loss: 0.02\n",
      "validation loss 0.72\n",
      "\n",
      "training loss: 0.02\n",
      "validation loss 0.72\n",
      "\n",
      "training loss: 0.02\n",
      "validation loss 0.72\n",
      "\n",
      "training loss: 0.02\n",
      "validation loss 0.72\n",
      "\n",
      "training loss: 0.02\n",
      "validation loss 0.72\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    with autograd.record():\n",
    "        output = net(features, A_)\n",
    "        l = loss_function(output[idx[train_mask]], nd.argmax(y_train[idx[train_mask]], axis = 1))\n",
    "    l.backward()\n",
    "    trainer.step(1)\n",
    "    print('training loss: %.2f'%(l.mean().asnumpy()[0]))\n",
    "    \n",
    "    output = net(features, A_)\n",
    "    l = loss_function(output[idx[val_mask]], nd.argmax(y_val[idx[val_mask]], axis = 1))\n",
    "    print('validation loss %.2f'%(l.mean().asnumpy()[0]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78800000000000003"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = net(features, A_)\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(np.argmax(y_test[idx[test_mask]], axis = 1), nd.argmax(output[idx[test_mask]], axis = 1).asnumpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Co-Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 50\n",
    "alpha = 1e-6\n",
    "Lambda = np.identity(len(A_))\n",
    "L = np.diag(D) - adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = inv(L + alpha * Lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = nd.argmax(y_train[idx[train_mask]], axis = 1).asnumpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx = set(idx[train_mask].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict = dict(zip(train_idx, map(int, train_label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(y_train.shape[1]):\n",
    "    nodes = idx[train_mask][train_label == k]\n",
    "    probability = P[:, nodes].sum(axis = 1).flatten()\n",
    "    for i in np.argsort(probability).tolist()[0][::-1][:t]:\n",
    "        if i in train_dict:\n",
    "            continue\n",
    "        train_dict[i] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "350"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_index = sorted(train_dict.keys())\n",
    "new_train_label = [train_dict[i] for i in new_train_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.95\n",
      "validation loss 1.93\n",
      "\n",
      "training loss: 1.91\n",
      "validation loss 1.92\n",
      "\n",
      "training loss: 1.88\n",
      "validation loss 1.90\n",
      "\n",
      "training loss: 1.84\n",
      "validation loss 1.89\n",
      "\n",
      "training loss: 1.81\n",
      "validation loss 1.87\n",
      "\n",
      "training loss: 1.77\n",
      "validation loss 1.85\n",
      "\n",
      "training loss: 1.73\n",
      "validation loss 1.83\n",
      "\n",
      "training loss: 1.69\n",
      "validation loss 1.81\n",
      "\n",
      "training loss: 1.65\n",
      "validation loss 1.79\n",
      "\n",
      "training loss: 1.61\n",
      "validation loss 1.77\n",
      "\n",
      "training loss: 1.56\n",
      "validation loss 1.74\n",
      "\n",
      "training loss: 1.51\n",
      "validation loss 1.72\n",
      "\n",
      "training loss: 1.46\n",
      "validation loss 1.69\n",
      "\n",
      "training loss: 1.41\n",
      "validation loss 1.66\n",
      "\n",
      "training loss: 1.36\n",
      "validation loss 1.63\n",
      "\n",
      "training loss: 1.30\n",
      "validation loss 1.60\n",
      "\n",
      "training loss: 1.25\n",
      "validation loss 1.57\n",
      "\n",
      "training loss: 1.19\n",
      "validation loss 1.54\n",
      "\n",
      "training loss: 1.14\n",
      "validation loss 1.50\n",
      "\n",
      "training loss: 1.08\n",
      "validation loss 1.47\n",
      "\n",
      "training loss: 1.02\n",
      "validation loss 1.44\n",
      "\n",
      "training loss: 0.97\n",
      "validation loss 1.40\n",
      "\n",
      "training loss: 0.91\n",
      "validation loss 1.37\n",
      "\n",
      "training loss: 0.86\n",
      "validation loss 1.34\n",
      "\n",
      "training loss: 0.80\n",
      "validation loss 1.31\n",
      "\n",
      "training loss: 0.75\n",
      "validation loss 1.27\n",
      "\n",
      "training loss: 0.70\n",
      "validation loss 1.24\n",
      "\n",
      "training loss: 0.66\n",
      "validation loss 1.21\n",
      "\n",
      "training loss: 0.61\n",
      "validation loss 1.18\n",
      "\n",
      "training loss: 0.57\n",
      "validation loss 1.15\n",
      "\n",
      "training loss: 0.53\n",
      "validation loss 1.13\n",
      "\n",
      "training loss: 0.49\n",
      "validation loss 1.10\n",
      "\n",
      "training loss: 0.45\n",
      "validation loss 1.07\n",
      "\n",
      "training loss: 0.42\n",
      "validation loss 1.05\n",
      "\n",
      "training loss: 0.39\n",
      "validation loss 1.03\n",
      "\n",
      "training loss: 0.36\n",
      "validation loss 1.01\n",
      "\n",
      "training loss: 0.33\n",
      "validation loss 0.99\n",
      "\n",
      "training loss: 0.31\n",
      "validation loss 0.97\n",
      "\n",
      "training loss: 0.29\n",
      "validation loss 0.95\n",
      "\n",
      "training loss: 0.27\n",
      "validation loss 0.93\n",
      "\n",
      "training loss: 0.25\n",
      "validation loss 0.92\n",
      "\n",
      "training loss: 0.23\n",
      "validation loss 0.91\n",
      "\n",
      "training loss: 0.21\n",
      "validation loss 0.89\n",
      "\n",
      "training loss: 0.20\n",
      "validation loss 0.88\n",
      "\n",
      "training loss: 0.18\n",
      "validation loss 0.87\n",
      "\n",
      "training loss: 0.17\n",
      "validation loss 0.86\n",
      "\n",
      "training loss: 0.16\n",
      "validation loss 0.85\n",
      "\n",
      "training loss: 0.15\n",
      "validation loss 0.84\n",
      "\n",
      "training loss: 0.14\n",
      "validation loss 0.83\n",
      "\n",
      "training loss: 0.13\n",
      "validation loss 0.83\n",
      "\n",
      "training loss: 0.12\n",
      "validation loss 0.82\n",
      "\n",
      "training loss: 0.12\n",
      "validation loss 0.82\n",
      "\n",
      "training loss: 0.11\n",
      "validation loss 0.81\n",
      "\n",
      "training loss: 0.10\n",
      "validation loss 0.81\n",
      "\n",
      "training loss: 0.10\n",
      "validation loss 0.80\n",
      "\n",
      "training loss: 0.09\n",
      "validation loss 0.80\n",
      "\n",
      "training loss: 0.09\n",
      "validation loss 0.80\n",
      "\n",
      "training loss: 0.08\n",
      "validation loss 0.79\n",
      "\n",
      "training loss: 0.08\n",
      "validation loss 0.79\n",
      "\n",
      "training loss: 0.07\n",
      "validation loss 0.79\n",
      "\n",
      "training loss: 0.07\n",
      "validation loss 0.79\n",
      "\n",
      "training loss: 0.07\n",
      "validation loss 0.79\n",
      "\n",
      "training loss: 0.06\n",
      "validation loss 0.78\n",
      "\n",
      "training loss: 0.06\n",
      "validation loss 0.78\n",
      "\n",
      "training loss: 0.06\n",
      "validation loss 0.78\n",
      "\n",
      "training loss: 0.06\n",
      "validation loss 0.78\n",
      "\n",
      "training loss: 0.05\n",
      "validation loss 0.78\n",
      "\n",
      "training loss: 0.05\n",
      "validation loss 0.78\n",
      "\n",
      "training loss: 0.05\n",
      "validation loss 0.78\n",
      "\n",
      "training loss: 0.05\n",
      "validation loss 0.78\n",
      "\n",
      "training loss: 0.04\n",
      "validation loss 0.78\n",
      "\n",
      "training loss: 0.04\n",
      "validation loss 0.78\n",
      "\n",
      "training loss: 0.04\n",
      "validation loss 0.77\n",
      "\n",
      "training loss: 0.04\n",
      "validation loss 0.77\n",
      "\n",
      "training loss: 0.04\n",
      "validation loss 0.77\n",
      "\n",
      "training loss: 0.04\n",
      "validation loss 0.77\n",
      "\n",
      "training loss: 0.04\n",
      "validation loss 0.77\n",
      "\n",
      "training loss: 0.03\n",
      "validation loss 0.77\n",
      "\n",
      "training loss: 0.03\n",
      "validation loss 0.77\n",
      "\n",
      "training loss: 0.03\n",
      "validation loss 0.77\n",
      "\n",
      "training loss: 0.03\n",
      "validation loss 0.77\n",
      "\n",
      "training loss: 0.03\n",
      "validation loss 0.77\n",
      "\n",
      "training loss: 0.03\n",
      "validation loss 0.77\n",
      "\n",
      "training loss: 0.03\n",
      "validation loss 0.77\n",
      "\n",
      "training loss: 0.03\n",
      "validation loss 0.77\n",
      "\n",
      "training loss: 0.03\n",
      "validation loss 0.78\n",
      "\n",
      "training loss: 0.03\n",
      "validation loss 0.78\n",
      "\n",
      "training loss: 0.03\n",
      "validation loss 0.78\n",
      "\n",
      "training loss: 0.03\n",
      "validation loss 0.78\n",
      "\n",
      "training loss: 0.02\n",
      "validation loss 0.78\n",
      "\n",
      "training loss: 0.02\n",
      "validation loss 0.78\n",
      "\n",
      "training loss: 0.02\n",
      "validation loss 0.78\n",
      "\n",
      "training loss: 0.02\n",
      "validation loss 0.78\n",
      "\n",
      "training loss: 0.02\n",
      "validation loss 0.78\n",
      "\n",
      "training loss: 0.02\n",
      "validation loss 0.78\n",
      "\n",
      "training loss: 0.02\n",
      "validation loss 0.78\n",
      "\n",
      "training loss: 0.02\n",
      "validation loss 0.78\n",
      "\n",
      "training loss: 0.02\n",
      "validation loss 0.78\n",
      "\n",
      "training loss: 0.02\n",
      "validation loss 0.78\n",
      "\n",
      "training loss: 0.02\n",
      "validation loss 0.78\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net2 = GCN()\n",
    "net2.initialize(ctx = ctx)\n",
    "net2.hybridize()\n",
    "\n",
    "trainer = gluon.Trainer(net2.collect_params(), 'adam', {'learning_rate': 1e-3})\n",
    "\n",
    "for epoch in range(100):\n",
    "    with autograd.record():\n",
    "        output = net2(features, A_)\n",
    "        l = loss_function(output[new_train_index], nd.array(new_train_label, ctx = ctx))\n",
    "    l.backward()\n",
    "    trainer.step(1)\n",
    "    print('training loss: %.2f'%(l.mean().asnumpy()[0]))\n",
    "    \n",
    "    output = net2(features, A_)\n",
    "    l = loss_function(output[idx[val_mask]], nd.argmax(y_val[idx[val_mask]], axis = 1))\n",
    "    print('validation loss %.2f'%(l.mean().asnumpy()[0]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78800000000000003"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = net(features, A_)\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(np.argmax(y_test[idx[test_mask]], axis = 1), nd.argmax(output[idx[test_mask]], axis = 1).asnumpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
